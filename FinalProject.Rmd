---
title: "FinalProject"
output: html_document
---

# An Analysis of Left Vs Right Reddit Posts

## Devon Sinha
## devon.sinha@duke.edu
## 4/18/2022

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

```{r, include=FALSE}
###Importing all the necessary libraries
library(tidyverse)
library(ggplot2)
library(stringr)
library(tidytext)
library(tm)
library(wordcloud)
library(wordcloud2)
library(RColorBrewer)
library(topicmodels)
```

```{r, message=FALSE, warning=FALSE}
##reading in our data
reddit<-read_csv("file_name.csv")

##just a quick look at our dataset
reddit %>% top_n(5)
```


```{r}
###Doing some EDA as Summary Statistics

### show frequency of liberals vs conservatives
toPlot<-reddit %>% group_by(`Political Lean`) %>% summarise(n=n())
ggplot(data=toPlot, aes(x=`Political Lean`, y=n,fill=`Political Lean`))+
  geom_bar(stat="identity") +
  scale_fill_manual(values=c(
                             "red",
                             "darkblue"))

reddit %>% group_by(Subreddit) %>% summarise(n=n())

```

We have posts from the subreddit groups: alltheleft, anarchocapitalism, Capitalism, Communist, conservatives, DemocraticSocialism, democrats, feminisms, Liberal, Libertarian, progressive, RadicalFeminism, republicans, SocialDemocracy, socialism.
```{r}
###amount of unique values here. Shows that each post is given its own unique ID
length(unique(reddit$Id))

hist(log(reddit$Score))

##IF I WANT I CAN COME BACK TO THIS AND ALSO DO THE HIST FOR LIBS AND CONS
```


```{r}
liberals<-reddit %>% filter(`Political Lean`=="Liberal")
conservatives<-reddit %>% filter(`Political Lean`=="Conservative")

```

```{r}
libPlot<-liberals %>% group_by(Subreddit) %>% summarise(n=n()) %>% arrange(desc(n))
consPlot<-conservatives %>% group_by(Subreddit) %>% summarise(n=n()) %>% arrange(desc(n))

libPlot

ggplot(libPlot, aes(reorder(Subreddit, -n ), n, fill=Subreddit))+geom_bar(stat="identity")+coord_flip()

ggplot(consPlot, aes(reorder(Subreddit, -n ), n, fill=Subreddit))+geom_bar(stat="identity")+coord_flip()
```


```{r}
###extract the key words from our title spts
library(stringr)
library(tidytext)
library(tm)
libCloud<-liberals %>% unnest_tokens('word', Title) %>% group_by(word) %>% count() %>% arrange(desc(n)) %>% anti_join(stop_words) %>% filter(nchar(word)>2)

consCloud<-conservatives %>% unnest_tokens('word', Title) %>% group_by(word) %>% count() %>% arrange(desc(n)) %>% anti_join(stop_words) %>% filter(nchar(word)>2)
```

```{r}
#install.packages("wordcloud") and install.packages("RColorBrewer") and install.packages("wordcloud2")
library(wordcloud)
library(wordcloud2)
library(RColorBrewer)
set.seed(1234) # for reproducibility 
wordcloud(words = libCloud$word, freq = libCloud$n, min.freq = 1, max.words=200, random.order=FALSE, rot.per=0.35, colors=brewer.pal(8, "Dark2"), scale=c(4,.2), main="title")
```
```{r}
wordcloud(words = consCloud$word, freq = consCloud$n, min.freq = 1, max.words=200, random.order=FALSE, rot.per=0.35, colors=brewer.pal(8, "Dark2"), scale=c(4,.3), main="title")
```

```{r warning=FALSE, message=FALSE}
##Do topic modeling here!!

libDtm<-liberals %>% unnest_tokens('word', Title) %>% anti_join(stop_words) %>%  filter(is.na(as.numeric(substr(word, 1, 1)))) %>% group_by(word) %>% count(Id,word) %>% cast_dtm(Id, word, n)

consDtm<-conservatives %>% unnest_tokens('word', Title) %>% anti_join(stop_words) %>%  filter(is.na(as.numeric(substr(word, 1, 1)))) %>% group_by(word) %>% count(Id,word) %>% cast_dtm(Id, word, n)

```

```{r}
##make sure to reorder the topics!!! also make sure to get scales on x-axis to fit. Might have to change size of the text
library(topicmodels)

libTm<-LDA(libDtm, k=10, control=list(seed=1111))
consTm<-LDA(consDtm, k=10, control=list(seed=2222))

libTopics<-tidy(libTm, matrix="beta")
consTopics<-tidy(consTm, matrix="beta")

libTop10<-libTopics %>% group_by(topic) %>% top_n(10, beta) %>% ungroup() %>% arrange(topic, -beta)
consTop10<-consTopics %>% group_by(topic) %>% top_n(10, beta) %>% ungroup() %>% arrange(topic, -beta)

libTop10 %>% mutate(term=reorder(term, beta)) %>%  mutate(topic=paste("Topic #", topic)) %>% ggplot(aes(term, beta, fill=factor(topic)))+geom_col(show.legend=FALSE)+facet_wrap(~ topic, scales="free")+theme_minimal()+theme(plot.title=element_text(hjust=.5, size=18))+labs(title="Top 10 Topics for Liberal's Posts on Reddit") + ylab("")+xlab("")+coord_flip()

consTop10 %>% mutate(term=reorder(term, beta)) %>%  mutate(topic=paste("Topic #", topic)) %>% ggplot(aes(term, beta, fill=factor(topic)))+geom_col(show.legend=FALSE)+facet_wrap(~ topic, scales="free")+theme_minimal()+theme(plot.title=element_text(hjust=.5, size=18))+labs(title="Top 10 Topics for Conservative's Posts on Reddit") + ylab("")+xlab("")+coord_flip()

```

```{r}
##Work on my logistic regression model here! 


```


```{r}
# reddit webscraper if possible
```


```{r}
## dictionaries for sentiment analysis
```

